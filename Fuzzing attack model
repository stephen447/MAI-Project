import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, regularizers
from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization
from keras.layers import Conv2D, MaxPooling2D, LeakyReLU, MaxPooling3D
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score
import sklearn as skl
import sklearn.model_selection
from sklearn.model_selection import train_test_split
from sklearn.dummy import DummyRegressor
from sklearn.utils import shuffle
import matplotlib.pyplot as plt
plt.rc('font', size=18)
plt.rcParams['figure.constrained_layout.use'] = True
import pandas as p
import sys
import ssl
import time
import sklearn.metrics as metrics
from sklearn.metrics import roc_curve
import matplotlib.patches as mpatches

start_time = time.time()
ssl._create_default_https_context = ssl._create_unverified_context

df = p.read_csv('/Users/stephenbyrne/Documents/College Year 5/Project/Attacks/fuzzing_attack_1.csv')
c1 = df.iloc[:, 2]  # Reading in ID

ID = [None]*len(c1)
PL = [None]*len(c1)
TV = np.zeros(len(c1))
y = 0; z = 0

for i in range(0, len(c1)):  # splitting id in HEX form
    string = c1[i]
    ID[i] = string[0:3]
    PL[i] = string[4:20]

for i in range(len(c1)):
	if c1[i][4:20] == "FFFFFFFFFFFFFFFF":
		TV[i] = 1
		y = y + 1
	else:
		TV[i] = 0
		z = z + 1

print('No. of attacks', y)
print('No. of non attacks', z)
ID1 = [None] * len(ID)  # Separated payload 1
ID2 = [None] * len(ID)  # Separated payload 2
ID3 = [None] * len(ID)  # Separated payload 3
IDbin = [None] * len(ID)  # Separated payload 3
#IDbin = np.zeros(len(ID))
IDcomb = [None] * len(ID)  # Separated payload 3

for i in range(0, len(ID)):
    id = ID[i]
    id = int(id, 16)
    id = bin(id)
    id = id[2:]
    id = id.zfill(12)
    IDbin[i] = id
#print(IDbin)

samples = len(c1)
IDs_in_past = 7
bit_length = 12

x = np.zeros([samples, IDs_in_past, bit_length])
y = np.zeros([samples, 1])

PL1 = [None] * len(c1)  #  Payload bit 16
PL2 = [None] * len(c1)  #  Payload bit 16
PL3 = [None] * len(c1)  #  Payload bit 16

for i in range(len(c1)):
    pl = c1[i]
    payload1 = pl[4:5]
    payload2 = pl[5:6]
    payload3 = pl[6:7]

    payload1 = int(payload1, 16)
    payload1 = bin(payload1)
    payload1 = payload1[2:]
    payload1 = payload1.zfill(12)

    payload2 = int(payload2, 16)
    payload2 = bin(payload2)
    payload2 = payload2[2:]
    payload2 = payload2.zfill(12)

    payload3 = int(payload3, 16)
    payload3 = bin(payload3)
    payload3 = payload3[2:]
    payload3 = payload3.zfill(12)

    PL1[i] = payload1
    PL2[i] = payload2
    PL3[i] = payload3

for i in range(3, samples):
    for j in range(IDs_in_past):
        for k in range(bit_length):
            temp = IDbin[i-j]
            x[i][j][k] = temp[k]

for i in range(3, len(c1)):
    for k in range(bit_length):
        temp1 = PL1[i]
        temp2 = PL2[i]
        temp3 = PL3[i]
        x[i][4][k] = temp1[k]
        x[i][5][k] = temp2[k]
        x[i][6][k] = temp3[k]

print(TV)
for i in range(3, samples):
	#print(TV[i])
	a = TV[i]
	b = TV[i - 1]
	c = TV[i - 2]
	d = TV[i - 3]
	z = np.sum([a, b, c, d])
	if(z<1):  # Normal
		y[i] = 0
	if(z >= 1):  # Attack
		y[i] = 1
		print('Attack!', i)

print('X is', x)
print('X shape', x.shape)
print('y is', y)
print('y shape', y.shape)

x = x.astype('i')
y = y.astype('i')

x_train, x_test, y_train, y_test = skl.model_selection.train_test_split(x, y, test_size=.1, random_state=0)  # Splitting data into training and test

# Model / data parameters
num_classes = 1
input_shape = (1, 7, 12)

print("orig x_train shape:", x_train.shape)
print("length of testing", len(x_test))
print("length of training", len(x_train))
print("x train", x_train)
print("y_train", y_train)

x_train = x_train.reshape(len(x_train), 1, 7, 12)
x_test = x_test.reshape(len(x_test), 1, 7, 12)
y_train = y_train.reshape(len(y_train), 1)
y_test = y_test.reshape(len(y_test), 1)

print('max train value', np.max(x_train))
print('Training values type', x_train.dtype)

use_saved_model = False
if use_saved_model:
	model = keras.models.load_model("cifar.model")
else:
	model = keras.Sequential()
	model.add(Conv2D(8, (3, 3), padding='same', input_shape=input_shape, activation='relu'))
	model.add(BatchNormalization())
	model.add(Dropout(0.2))
	#model.add(MaxPooling3D(pool_size=(2, 2, 2)))

	model.add(Conv2D(16, (3, 3), padding='same', activation='relu'))
	model.add(BatchNormalization())
	model.add(Dropout(0.2))
	#model.add(MaxPooling2D(pool_size=(2, 2)))

	model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))
	model.add(BatchNormalization())
	model.add(Dropout(0.2))
	#model.add(MaxPooling2D(pool_size=(2, 2)))

	model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))
	model.add(BatchNormalization())
	model.add(Dropout(0.2))
	#model.add(MaxPooling2D(pool_size=(2, 2)))

	model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))
	model.add(BatchNormalization())
	model.add(Dropout(0.2))
	# model.add(MaxPooling2D(pool_size=(2, 2)))

	#model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))
	#model.add(BatchNormalization())
	#model.add(Dropout(0.2))
	#model.add(MaxPooling2D(pool_size=(2, 2)))


	# model.add(MaxPooling2D(pool_size=(2, 2)))

	model.add(Flatten())
	model.add(Dense(16, activation='relu'))
	model.add(Dense(num_classes, activation='sigmoid', kernel_regularizer=regularizers.l1(.001)))
	# model.compile(loss="sparse_categorical_crossentropy", optimizer='adam', metrics=["accuracy"])
	model.compile(loss="binary_crossentropy", optimizer='adam', metrics=["accuracy"])
	model.summary()

	batch_size = 256
	epochs = 10
	history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)
	model.save("cifar.model")
	print("--- %s seconds ---" % (time.time() - start_time))
	plt.subplot(211)
	plt.plot(history.history['accuracy'])
	plt.plot(history.history['val_accuracy'])
	plt.title('model accuracy')
	plt.ylabel('accuracy')
	plt.xlabel('epoch')
	plt.legend(['train', 'val'], loc='upper left')
	plt.subplot(212)
	plt.plot(history.history['loss'])
	plt.plot(history.history['val_loss'])
	plt.title('model loss')
	plt.ylabel('loss'); plt.xlabel('epoch')
	plt.legend(['train', 'val'], loc='upper left')
	plt.show()

# Classifying data
np.set_printoptions(threshold=sys.maxsize)
preds = model.predict(x_test)
print('Predictions', preds)
for i in range(len(preds)):
	if(preds[i]>.5):
		#print(preds[i], '\n', i)
		preds[i] = 1
	else:
		preds[i] = 0


# Performance metrics
print("X test", x_test)
print("y test", y_test)
print("predictions", preds)
print(classification_report(y_test, preds))
print(confusion_matrix(y_test,preds))
#print("Accuracy (Baseline):  ", model.score(t, BL_pred))  # Accuracy
print("F1 score: ", f1_score(y_test, preds, average="macro"))  # F1 score
print("Precision: ", precision_score(y_test, preds, average="macro"))  # Precision score
print("Recall: ", recall_score(y_test, preds, average="macro"))  # Recall score

# ROC curve
plt.figure('ROC curve')
fpr, tpr, _ = roc_curve(y_test, preds)  # Defining false positives and true positives
plt.title("ROC curve for NN") # Title
plt.plot(fpr,tpr, c='blue')  # Plot roc curve
plt.xlabel('False positive rate')  # X label
plt.ylabel('True positive rate')  # Y label
#plt.show()

# Calculate the fpr and tpr for all thresholds of the classification
probs = model.predict(x_test)
predicts = probs[:,0]
fpr, tpr, threshold = roc_curve(y_test, predicts)  # Knn fpr and tpr
roc_auc = metrics.auc(fpr, tpr)
plt.plot(fpr, tpr, c='orange')  # Plot roc curve

NN = mpatches.Patch(color = 'green', label="LR")  # Legend for NN
plt.legend(handles=[NN], loc="center right")  # Plotting legend
fpr, tpr, thresholds = roc_curve(y_test, preds)  # ROC curve
plt.plot(fpr, tpr, c='green')  # Plot ROC curve
plt.show()
